{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 14 – Natural Language Processing with RNNs and Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 14._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ageron/handson-mlp/blob/main/14_nlp_with_rnns_and_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-mlp/blob/main/14_nlp_with_rnns_and_attention.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFXIv9qNpKzt",
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires Python 3.10 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJtVEqxfpKzw"
   },
   "source": [
    "And PyTorch ≥ 2.4.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0Piq5se2pKzx"
   },
   "outputs": [],
   "source": [
    "from packaging.version import Version\n",
    "import torch\n",
    "\n",
    "assert Version(torch.__version__) >= Version(\"2.4.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Colab, a few libraries are not pre-installed so we must install them manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in sys.modules:\n",
    "    %pip install -q torchmetrics\n",
    "    %pip install -qU datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDaDoLQTpKzx"
   },
   "source": [
    "As we did in earlier chapters, let's define the default font sizes to make the figures prettier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8d4TH3NbpKzx"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter can be very slow without a hardware accelerator, so if we can find one, let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's issue a warning if there's no hardware accelerator available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "if device == \"cpu\":\n",
    "    print(\"Neural nets can be very slow without a hardware accelerator.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
    "              \"accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcoUIRsvpKzy"
   },
   "source": [
    "And let's create the `images/nlp` folder (if it doesn't already exist), and define the `save_fig()` function which is used through this notebook to save the figures in high-res for the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PQFH5Y9PpKzy"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"nlp\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Shakespearean Text Using a Character RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the Shakespeare data from Andrej Karpathy's [char-rnn project](https://github.com/karpathy/char-rnn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "def download_shakespeare_text():\n",
    "    path = Path(\"datasets/shakespeare/shakespeare.txt\")\n",
    "    if not path.is_file():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://homl.info/shakespeare\"\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "    return path.read_text()\n",
    "\n",
    "shakespeare_text = download_shakespeare_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "# extra code – shows a short text sample\n",
    "print(shakespeare_text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(shakespeare_text.lower()))\n",
    "\"\".join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_id = {char: index for index, char in enumerate(vocab)}\n",
    "id_to_char = {index: char for index, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_id[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_char[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def encode_text(text):\n",
    "    return torch.tensor([char_to_id[char] for char in text.lower()])\n",
    "\n",
    "def decode_text(char_ids):\n",
    "    return \"\".join([id_to_char[char_id.item()] for char_id in char_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20, 17, 24, 24, 27,  6,  1, 35, 27, 30, 24, 16,  2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = encode_text(\"Hello, world!\")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, world!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_text(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text, window_length):\n",
    "        self.encoded_text = encode_text(text)\n",
    "        self.window_length = window_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text) - self.window_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"dataset index out of range\")\n",
    "        end = idx + self.window_length\n",
    "        window = self.encoded_text[idx : end]\n",
    "        target = self.encoded_text[idx + 1 : end + 1]\n",
    "        return window, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([32, 27,  1, 14, 17,  1, 27, 30,  1, 26]), y=tensor([27,  1, 14, 17,  1, 27, 30,  1, 26, 27])\n",
      "    decoded: x='to be or n', y='o be or no'\n",
      "x=tensor([27,  1, 14, 17,  1, 27, 30,  1, 26, 27]), y=tensor([ 1, 14, 17,  1, 27, 30,  1, 26, 27, 32])\n",
      "    decoded: x='o be or no', y=' be or not'\n",
      "x=tensor([ 1, 14, 17,  1, 27, 30,  1, 26, 27, 32]), y=tensor([14, 17,  1, 27, 30,  1, 26, 27, 32,  1])\n",
      "    decoded: x=' be or not', y='be or not '\n",
      "x=tensor([14, 17,  1, 27, 30,  1, 26, 27, 32,  1]), y=tensor([17,  1, 27, 30,  1, 26, 27, 32,  1, 32])\n",
      "    decoded: x='be or not ', y='e or not t'\n",
      "x=tensor([17,  1, 27, 30,  1, 26, 27, 32,  1, 32]), y=tensor([ 1, 27, 30,  1, 26, 27, 32,  1, 32, 27])\n",
      "    decoded: x='e or not t', y=' or not to'\n",
      "x=tensor([ 1, 27, 30,  1, 26, 27, 32,  1, 32, 27]), y=tensor([27, 30,  1, 26, 27, 32,  1, 32, 27,  1])\n",
      "    decoded: x=' or not to', y='or not to '\n",
      "x=tensor([27, 30,  1, 26, 27, 32,  1, 32, 27,  1]), y=tensor([30,  1, 26, 27, 32,  1, 32, 27,  1, 14])\n",
      "    decoded: x='or not to ', y='r not to b'\n",
      "x=tensor([30,  1, 26, 27, 32,  1, 32, 27,  1, 14]), y=tensor([ 1, 26, 27, 32,  1, 32, 27,  1, 14, 17])\n",
      "    decoded: x='r not to b', y=' not to be'\n"
     ]
    }
   ],
   "source": [
    "# extra code – a simple example using CharDataset\n",
    "to_be_dataset = CharDataset(\"To be or not to be\", window_length=10)\n",
    "for x, y in to_be_dataset:\n",
    "    print(f\"x={x}, y={y}\")\n",
    "    print(f\"    decoded: x={decode_text(x)!r}, y={decode_text(y)!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 50\n",
    "batch_size = 512  # reduce if your GPU cannot handle such a large batch size\n",
    "train_set = CharDataset(shakespeare_text[:1_000_000], window_length)\n",
    "valid_set = CharDataset(shakespeare_text[1_000_000:1_060_000], window_length)\n",
    "test_set = CharDataset(shakespeare_text[1_060_000:], window_length)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2674,  0.5349,  0.8094],\n",
       "         [ 2.2082, -0.6380,  0.4617]],\n",
       "\n",
       "        [[ 0.3367,  0.1288,  0.2345],\n",
       "         [ 2.2082, -0.6380,  0.4617]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "embed = nn.Embedding(5, 3)  # 5 categories × 3D embeddings\n",
    "embed(torch.tensor([[3, 2], [0, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training the Char-RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same `evaluate_tm()` and `train()` functions as in the previous chapters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "def evaluate_tm(model, data_loader, metric):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            metric.update(y_pred, y_batch)\n",
    "    return metric.compute()\n",
    "\n",
    "def train(model, optimizer, loss_fn, metric, train_loader, valid_loader,\n",
    "          n_epochs, patience=2, factor=0.5, epoch_callback=None):\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", patience=patience, factor=factor)\n",
    "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "        metric.reset()\n",
    "        model.train()\n",
    "        if epoch_callback is not None:\n",
    "            epoch_callback(model, epoch)\n",
    "        for index, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            metric.update(y_pred, y_batch)\n",
    "            train_metric = metric.compute().item()\n",
    "            print(f\"\\rBatch {index + 1}/{len(train_loader)}\", end=\"\")\n",
    "            print(f\", loss={total_loss/(index+1):.4f}\", end=\"\")\n",
    "            print(f\", {train_metric=:.2%}\", end=\"\")\n",
    "        history[\"train_losses\"].append(total_loss / len(train_loader))\n",
    "        history[\"train_metrics\"].append(train_metric)\n",
    "        val_metric = evaluate_tm(model, valid_loader, metric).item()\n",
    "        history[\"valid_metrics\"].append(val_metric)\n",
    "        scheduler.step(val_metric)\n",
    "        print(f\"\\rEpoch {epoch + 1}/{n_epochs},                      \"\n",
    "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
    "              f\"train metric: {history['train_metrics'][-1]:.2%}, \"\n",
    "              f\"valid metric: {history['valid_metrics'][-1]:.2%}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our Shakespeare model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers=2, embedding_size=10, n_hidden=128,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, n_hidden, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear(n_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        embeddings = self.embed(X)\n",
    "        outputs, _states = self.gru(embeddings)\n",
    "        return self.output(outputs).permute(0, 2, 1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = ShakespeareModel(len(vocab)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: the following code may take a while to run, especially without a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20,                      train loss: 1.6035, train metric: 51.30%, valid metric: 51.84%\n",
      "Epoch 2/20,                      train loss: 1.3851, train metric: 56.71%, valid metric: 52.83%\n",
      "Epoch 3/20,                      train loss: 1.3555, train metric: 57.45%, valid metric: 53.67%\n",
      "Epoch 4/20,                      train loss: 1.3413, train metric: 57.80%, valid metric: 53.22%\n",
      "Epoch 5/20,                      train loss: 1.3328, train metric: 58.02%, valid metric: 53.80%\n",
      "Epoch 6/20,                      train loss: 1.3272, train metric: 58.15%, valid metric: 54.06%\n",
      "Epoch 7/20,                      train loss: 1.3232, train metric: 58.25%, valid metric: 53.66%\n",
      "Epoch 8/20,                      train loss: 1.3201, train metric: 58.33%, valid metric: 54.26%\n",
      "Epoch 9/20,                      train loss: 1.3176, train metric: 58.39%, valid metric: 54.45%\n",
      "Epoch 10/20,                      train loss: 1.3156, train metric: 58.44%, valid metric: 54.16%\n",
      "Epoch 11/20,                      train loss: 1.3140, train metric: 58.48%, valid metric: 54.55%\n",
      "Epoch 12/20,                      train loss: 1.3126, train metric: 58.52%, valid metric: 54.51%\n",
      "Epoch 13/20,                      train loss: 1.3112, train metric: 58.54%, valid metric: 54.49%\n",
      "Epoch 14/20,                      train loss: 1.3102, train metric: 58.57%, valid metric: 54.40%\n",
      "Epoch 15/20,                      train loss: 1.3016, train metric: 58.82%, valid metric: 55.04%\n",
      "Epoch 16/20,                      train loss: 1.3002, train metric: 58.86%, valid metric: 54.92%\n",
      "Epoch 17/20,                      train loss: 1.2996, train metric: 58.87%, valid metric: 54.76%\n",
      "Epoch 18/20,                      train loss: 1.2990, train metric: 58.88%, valid metric: 54.78%\n",
      "Epoch 19/20,                      train loss: 1.2942, train metric: 59.03%, valid metric: 55.06%\n",
      "Epoch 20/20,                      train loss: 1.2936, train metric: 59.04%, valid metric: 55.02%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "xentropy = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\",\n",
    "                                 num_classes=len(vocab)).to(device)\n",
    "\n",
    "history = train(model, optimizer, xentropy, accuracy, train_loader, valid_loader,\n",
    "                n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"my_shakespeare_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # don't forget to switch the model to evaluation mode!\n",
    "text = \"To be or not to b\"\n",
    "encoded_text = encode_text(text).unsqueeze(dim=0).to(device)\n",
    "with torch.no_grad():\n",
    "    Y_logits = model(encoded_text)\n",
    "    predicted_char_id = Y_logits[0, :, -1].argmax().item()\n",
    "    predicted_char = id_to_char[predicted_char_id]  # correctly predicts \"e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Shakespearean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1, 0, 2, 2]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "probs = torch.tensor([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
    "samples = torch.multinomial(probs, replacement=True, num_samples=8)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def next_char(model, text, temperature=1):\n",
    "    encoded_text = encode_text(text).unsqueeze(dim=0).to(device)\n",
    "    with torch.no_grad():\n",
    "        Y_logits = model(encoded_text)\n",
    "        Y_probas = F.softmax(Y_logits[0, :, -1] / temperature, dim=-1)\n",
    "        predicted_char_id = torch.multinomial(Y_probas, num_samples=1).item()\n",
    "    return id_to_char[predicted_char_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(model, text, n_chars=80, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(model, text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be the state\n",
      "and the contrary of the state and the sea,\n",
      "the common people of the \n"
     ]
    }
   ],
   "source": [
    "print(extend_text(model, \"To be or not to b\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be the better from the cause\n",
      "that thou think you may be so be gone.\n",
      "\n",
      "romeo:\n",
      "that \n"
     ]
    }
   ],
   "source": [
    "print(extend_text(model, \"To be or not to b\", temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to b-c3;m-rkn&:x:uyve:b&hi n;n-h;wt3k\n",
      "&cixxh:a!kq$c$ 3 ncq$ ;;wq cp:!xq;yh\n",
      "!3\n",
      "d!nhi.\n"
     ]
    }
   ],
   "source": [
    "print(extend_text(model, \"To be or not to b\", temperature=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a stateful RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we have only used _stateless RNNs_: at each training iteration the model starts with a hidden state full of zeros, then it updates this state at each time step, and after the last time step, it throws the state away as it is not needed anymore. What if we instructed the RNN to preserve this final state after processing a training batch and use it as the initial state for the next training batch? This way the model could learn long-term patterns despite only backpropagating through short sequences. This is called a _stateful RNN_. Let's go over how to build one.\n",
    "\n",
    "The model itself requires very little change: we only need to add a new `hidden_states` attribute, initialized to `None`, then save the hidden states after each batch is processed, and use them as the initial hidden states for the next batch. Note that we must call `detach()` on these states to ensure we don't backpropagate over this training iteration's computation graph at the next iteration (this would cause an error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulShakespeareModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers=2, embedding_size=10, n_hidden=128,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.gru = nn.GRU(embedding_size, n_hidden, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear(n_hidden, vocab_size)\n",
    "        self.hidden_states = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        embeddings = self.embed(X)\n",
    "        outputs, hidden_states = self.gru(embeddings, self.hidden_states)\n",
    "        self.hidden_states = hidden_states.detach()\n",
    "        return self.output(outputs).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difficulty with stateful RNNs is preparing the dataset. Indeed, a stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous batch left off. To be more precise, the _n_<sup>th</sup> window in batch _k_ must start exactly where the _n_<sup>th</sup> window in batch _k_ – 1 stopped. For example, suppose the full encoded text is `[1, 2, 3, .., 59, 60, 61]` and you want to use a window length of 4, and a batch size of 5. The dataset could contain 3 batches like these, in this order:\n",
    "\n",
    "```\n",
    "Batch #1:\n",
    "X=[[1,2,3,4], [13,14,15,16], [25,26,27,28], [37,38,39,40], [49,50,51,52]]\n",
    "Y=[[2,3,4,5], [14,15,16,17], [26,27,28,29], [38,39,40,41], [50,51,52,53]]\n",
    "\n",
    "Batch #2:\n",
    "X=[[5,6,7,8], [17,18,19,20], [29,30,31,32], [41,42,43,44], [53,54,55,56]]\n",
    "y=[[6,7,8,9], [18,19,20,21], [30,31,32,33], [42,43,44,45], [54,55,56,57]]\n",
    "\n",
    "Batch #3:\n",
    "X=[[9,10,11,12], [21,22,23,24], [33,34,35,36], [45,46,47,48], [57,58,59,60]]\n",
    "y=[[10,11,12,13], [22,23,24,25], [34,35,36,37], [46,47,48,49], [58,59,60,61]]\n",
    "```\n",
    "\n",
    "Let's write a `StatefulCharDataset` class that organizes the data like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StatefulCharDataset(Dataset):\n",
    "    def __init__(self, text, window_length, batch_size):\n",
    "        self.encoded_text = encode_text(text)\n",
    "        self.window_length = window_length\n",
    "        self.batch_size = batch_size\n",
    "        n_consecutive_windows = (len(self.encoded_text) - 1) // window_length\n",
    "        n_windows_per_slot = n_consecutive_windows // batch_size\n",
    "        self.length = n_windows_per_slot * batch_size\n",
    "        self.spacing = n_windows_per_slot * window_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"dataset index out of range\")\n",
    "        start = ((idx % self.batch_size) * self.spacing\n",
    "                 +(idx // self.batch_size) * self.window_length)\n",
    "        end = start + self.window_length\n",
    "        window = self.encoded_text[start : end]\n",
    "        target = self.encoded_text[start + 1 : end + 1]\n",
    "        return window, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the data loaders. Note that we must *not* shuffle the batches, even for the training set. We must also ensure that all batches have exactly the same number of windows, even the very last batch: for this reason, we must set `drop_last=True` when creating the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "stateful_train_set = StatefulCharDataset(shakespeare_text[:1_000_000],\n",
    "                                         window_length, batch_size)\n",
    "stateful_train_loader = DataLoader(stateful_train_set, batch_size=batch_size,\n",
    "                                   drop_last=True)\n",
    "stateful_valid_set = StatefulCharDataset(shakespeare_text[1_000_000:1_060_000],\n",
    "                                         window_length, batch_size)\n",
    "stateful_valid_loader = DataLoader(stateful_valid_set, batch_size=batch_size,\n",
    "                                   drop_last=True)\n",
    "stateful_test_set = StatefulCharDataset(shakespeare_text[1_060_000:],\n",
    "                                        window_length, batch_size)\n",
    "stateful_test_loader = DataLoader(stateful_test_set, batch_size=batch_size,\n",
    "                                  drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we should reset the hidden states at the start of each epoch. We could rewrite the whole training loop just for that, but it's cleaner to just pass a callback function to the `train()` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: the following cell may take a long time to run, especially without a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10,                      train loss: 2.4736, train metric: 29.84%, valid metric: 39.19%\n",
      "Epoch 2/10,                      train loss: 1.8788, train metric: 44.47%, valid metric: 45.18%\n",
      "Epoch 3/10,                      train loss: 1.6925, train metric: 49.30%, valid metric: 48.28%\n",
      "Epoch 4/10,                      train loss: 1.5978, train metric: 51.71%, valid metric: 49.84%\n",
      "Epoch 5/10,                      train loss: 1.5416, train metric: 53.16%, valid metric: 51.01%\n",
      "Epoch 6/10,                      train loss: 1.5034, train metric: 54.10%, valid metric: 51.79%\n",
      "Epoch 7/10,                      train loss: 1.4751, train metric: 54.83%, valid metric: 52.46%\n",
      "Epoch 8/10,                      train loss: 1.4538, train metric: 55.33%, valid metric: 52.89%\n",
      "Epoch 9/10,                      train loss: 1.4360, train metric: 55.81%, valid metric: 53.17%\n",
      "Epoch 10/10,                      train loss: 1.4222, train metric: 56.11%, valid metric: 53.81%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "stateful_model = StatefulShakespeareModel(len(vocab)).to(device)\n",
    "\n",
    "n_epochs = 10\n",
    "xentropy = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.NAdam(stateful_model.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\",\n",
    "                                 num_classes=len(vocab)).to(device)\n",
    "\n",
    "def reset_hidden_states(model, epoch):\n",
    "    model.hidden_states = None\n",
    "\n",
    "history = train(stateful_model, optimizer, xentropy, accuracy, stateful_train_loader,\n",
    "                stateful_valid_loader, n_epochs, epoch_callback=reset_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(stateful_model.state_dict(), \"my_stateful_shakespeare_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try generating some text with our stateful RNN. We must reset the `hidden_states` every time we call the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text_with_stateful_rnn(model, text, n_chars=80, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        model.hidden_states = None\n",
    "        text += next_char(model, text, temperature)\n",
    "    return text + \"…\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be\n",
      "that the senate the senate the senate the seather.\n",
      "\n",
      "clarence:\n",
      "i shall be the d…\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "stateful_model.eval()\n",
    "print(extend_text_with_stateful_rnn(stateful_model, \"To be or not to b\",\n",
    "                                    temperature=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be\n",
      "the body and the commanding to the duke\n",
      "that he shall be so did not side,\n",
      "wher…\n"
     ]
    }
   ],
   "source": [
    "print(extend_text(stateful_model, \"To be or not to b\", temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be.\n",
      "\n",
      "mortam:\n",
      "will abless must; that chuses, as cack noble;\n",
      "with a life that hath.…\n"
     ]
    }
   ],
   "source": [
    "print(extend_text(stateful_model, \"To be or not to b\", temperature=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "split = imdb_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "imdb_train_set, imdb_valid_set = split[\"train\"], split[\"test\"]\n",
    "imdb_test_set = imdb_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'The Rookie' was a wonderful movie about the second chances life holds for us and also puts an emotional thought over the audience, making them realize that your dreams can come true. If you loved 'Remember the Titans', 'The Rookie' is the movie for you!! It's the feel good movie of the year and it is the perfect movie for all ages. 'The Rookie' hits a major home run!\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train_set[1][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train_set[1][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lillian Hellman's play, adapted by Dashiell Hammett with help from Hellman, becomes a curious project to come out of gritty Warner Bros. Paul Lukas, reprising his Broadway role and winning the Best Actor Oscar, plays an anti-Nazi German underground leader fighting the Fascists, dragging his American wife and three children all over Europe before finding refuge in the States (via the Mexico border). They settle in Washington with the wife's wealthy mother and brother, though a boarder residing in the manor is immediately suspicious of the newcomers and spends an awful lot of time down at the German Embassy playing poker. It seems to take forever for this drama to find its focus, and when we realize what the heart of the material is (the wise, honest, direct refugees teaching the clueless, head-in-the-sand Americans how the world has suddenly changed), it seems a little patronizing--the viewer is quite literally put in the relatives' place, being lectured to. Lukas has several speeches in the third-act which undoubtedly won him the Academy Award, yet for the much of the picture he seems to do little but enter and exit, enter and exit. As his spouse, Bette Davis enunciates like nobody else and works her wide eyes to good advantage, but the role doesn't allow her much color. Their children (all with divergent accents!) are alternately humorous and annoying, and Geraldine Fitzgerald has a nothing role as a put-upon wife (and the disgruntled texture she brings to the part seems entirely wrong). The intent here was to tastefully, tactfully show us just because a (WWII-era) man may be German, that doesn't make him a Nazi sympathizer. We get that in the first few minutes; the rest of this tasteful, tactful movie is made up of exposition, defensive confrontation and, ultimately, compassion. It should be a heady mix, but instead it's rather dry-eyed and inert. ** from ****\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train_set[16][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train_set[16][\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Using the `tokenizers` Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a BPE Tokenizer on the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tokenizers\n",
    "\n",
    "bpe_model = tokenizers.models.BPE(unk_token=\"<unk>\")\n",
    "bpe_tokenizer = tokenizers.Tokenizer(bpe_model)\n",
    "bpe_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "special_tokens = [\"<pad>\", \"<unk>\"]\n",
    "bpe_trainer = tokenizers.trainers.BpeTrainer(vocab_size=1000,\n",
    "                                             special_tokens=special_tokens)\n",
    "train_reviews = [review[\"text\"].lower() for review in imdb_train_set]\n",
    "bpe_tokenizer.train_from_iterator(train_reviews, bpe_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)), (',', (5, 6)), ('world', (7, 12)), ('!!!', (12, 15))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers.pre_tokenizers.Whitespace().pre_tokenize_str(\"Hello, world!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Decoding Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_review = \"what an awesome movie! 😊\"\n",
    "bpe_encoding = bpe_tokenizer.encode(some_review)\n",
    "bpe_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'an', 'aw', 'es', 'ome', 'movie', '!', '<unk>']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[303, 139, 373, 149, 240, 211, 4, 1]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_token_ids = bpe_encoding.ids\n",
    "bpe_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.get_vocab()[\"what\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.token_to_id(\"what\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ough'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.id_to_token(305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what an aw es ome movie !'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.decode(bpe_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4), (5, 7), (8, 10), (10, 12), (12, 15), (16, 21), (21, 22), (23, 24)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_encoding.offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=281, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=114, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=285, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode_batch(train_reviews[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
    "bpe_tokenizer.enable_truncation(max_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[159, 402, 176, 246,  61, [...], 215, 156, 586,   0,   0,   0,   0],\n",
       "        [ 10, 138, 198, 289, 175, [...],   0,   0,   0,   0,   0,   0,   0],\n",
       "        [289,  15, 209, 398, 177, [...],  50,  29,  22,  17,  24,  18,  24]])\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_encodings = bpe_tokenizer.encode_batch(train_reviews[:3])\n",
    "bpe_batch_ids = torch.tensor([encoding.ids for encoding in bpe_encodings])\n",
    "bpe_batch_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, [...], 1, 1, 1, 1, 1, 1, 1]])\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.tensor([encoding.attention_mask\n",
    "                               for encoding in bpe_encodings])\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([281, 114, 285])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = attention_mask.sum(dim=-1)\n",
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBPE Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 😊 emoji is represented as 4 bytes when using the UTF-8 Unicode encoding, so the `ByteLevel` pre-tokenizer represents it as 4 characters, each representing a byte. Spaces are converted to Ġ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ġwhat', (0, 4)),\n",
       " ('Ġan', (4, 7)),\n",
       " ('Ġawesome', (7, 15)),\n",
       " ('Ġmovie', (15, 21)),\n",
       " ('!', (21, 22)),\n",
       " ('ĠðŁĺĬ', (22, 24))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers.pre_tokenizers.ByteLevel().pre_tokenize_str(some_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bbpe_model = tokenizers.models.BPE(unk_token=\"<unk>\")\n",
    "bbpe_tokenizer = tokenizers.Tokenizer(bbpe_model)\n",
    "bbpe_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel()\n",
    "bbpe_trainer = tokenizers.trainers.BpeTrainer(vocab_size=1000,\n",
    "                                              special_tokens=special_tokens)\n",
    "bbpe_tokenizer.train_from_iterator(train_reviews, bbpe_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġwhat',\n",
       " 'Ġan',\n",
       " 'Ġaw',\n",
       " 'es',\n",
       " 'ome',\n",
       " 'Ġmovie',\n",
       " '!',\n",
       " 'Ġ',\n",
       " '<unk>',\n",
       " 'Ł',\n",
       " 'ĺ',\n",
       " '<unk>']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbpe_encoding = bbpe_tokenizer.encode(some_review)\n",
    "bbpe_tokens = bbpe_encoding.tokens\n",
    "bbpe_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[354, 216, 561, 148, 244, 232, 2, 107, 1, 125, 119, 1]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbpe_token_ids = bbpe_encoding.ids\n",
    "bbpe_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġwhat Ġan Ġaw es ome Ġmovie ! Ġ Ł ĺ'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbpe_decoded = bbpe_tokenizer.decode(bbpe_token_ids)\n",
    "bbpe_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what an awesome movie! Łĺ'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbpe_decoded.replace(\" \", \"\").replace(\"Ġ\", \" \").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wp_model = tokenizers.models.WordPiece(unk_token=\"<unk>\")\n",
    "wp_tokenizer = tokenizers.Tokenizer(wp_model)\n",
    "wp_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "wp_trainer = tokenizers.trainers.WordPieceTrainer(vocab_size=1000,\n",
    "                                                  special_tokens=special_tokens)\n",
    "wp_tokenizer.train_from_iterator(train_reviews, wp_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'an', 'aw', '##es', '##ome', 'movie', '!', '<unk>']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_encoding = wp_tokenizer.encode(some_review)\n",
    "wp_tokens = wp_encoding.tokens\n",
    "wp_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[443, 312, 635, 257, 354, 331, 4, 1]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_token_ids = wp_encoding.ids\n",
    "wp_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what an aw ##es ##ome movie !'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_decoded = wp_tokenizer.decode(wp_token_ids)\n",
    "wp_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what an awesome movie!'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_decoded.replace(\" ##\", \"\").replace(\" !\", \"!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unigram_model = tokenizers.models.Unigram()\n",
    "unigram_tokenizer = tokenizers.Tokenizer(unigram_model)\n",
    "unigram_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "unigram_trainer = tokenizers.trainers.UnigramTrainer(\n",
    "    vocab_size=1000, special_tokens=special_tokens, unk_token=\"<unk>\")\n",
    "unigram_tokenizer.train_from_iterator(train_reviews, unigram_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'an', 'a', 'w', 'e', 'some', 'movie', '!', '😊']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_encoding = unigram_tokenizer.encode(some_review)\n",
    "unigram_tokens = unigram_encoding.tokens\n",
    "unigram_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[79, 37, 4, 40, 6, 70, 46, 74, 1]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_token_ids = unigram_encoding.ids[:10]\n",
    "unigram_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what an a w e some movie !'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tokenizer.decode(unigram_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BBPE is used by models like GPT-2 and RoBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_encoding = gpt2_tokenizer(train_reviews[:3], truncation=True,\n",
    "                               max_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14247, 35030, 1690, 423, 257, 1688, 8046, 13, 484, 1690]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_token_ids = gpt2_encoding[\"input_ids\"][0][:10]\n",
    "gpt2_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stage adaptations often have a major fault. they often'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_tokenizer.decode(gpt2_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordPiece is used by models like BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_encoding = bert_tokenizer(train_reviews[:3], padding=True,\n",
    "                               truncation=True, max_length=500,\n",
    "                               return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2754, 17241,  2411,  2031,  1037,  2350,  6346,  1012,  2027,\n",
       "          2411,  2272,  2041,  2559,  2066,  1037,  2143,  4950,  2001,  3432,\n",
       "          2872,  2006,  1996,  2754,  1006,  2107,  2004,  1000,  2305,  2388,\n",
       "          1000,  1007,  1012, 11430, 11320, 11368,  1005,  1055,  3257,  7906,\n",
       "          1996,  2143,  4142,  1010,  2029,  2003,  2926,  3697,  2144,  1996,\n",
       "          3861,  3253,  2032,  2053,  2613,  4119,  1012,  2145,  1010,  2009,\n",
       "          1005,  1055,  3835,  2000,  2298,  2012,  2005,  2054,  2009,  2003,\n",
       "          1012,  1996,  6370,  2090,  2745, 19881,  1998,  5696, 20726,  2003,\n",
       "          3243,  8235,  1012,  1996, 10949,  1997,  2037,  3276,  2024, 11341,\n",
       "          1012, 19881,  2003, 10392,  2004,  2467,  1010,  1998, 20726,  4152,\n",
       "          2028,  1997,  2010,  2261,  9592,  2000,  2428,  2552,  1012,  1026,\n",
       "          7987,  1013,  1028,  1026,  7987,  1013,  1028,  1045, 18766,  2008,\n",
       "          1045,  1005,  2310,  2196,  2464, 11209, 20206,  1005,  1055,  2377,\n",
       "          1010,  2021,  1045,  2963,  2008,  6108,  2811,  2239,  5297,  1005,\n",
       "          1055,  6789,  2003, 11633,  1012,  1996,  5896,  2003, 11757,  9530,\n",
       "          6767,  7630,  3064,  1010,  1998,  7906,  2017, 16986,  1012,  1000,\n",
       "          2331,  6494,  2361,  1000,  2003,  2019,  8216,  2135, 14036,  2143,\n",
       "          1010,  1998,  2003,  6749,  2005,  3053,  2035,  4599,  1997,  2754,\n",
       "          1998,  3898,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
       "          1028,  1021,  1012,  1018,  2041,  1997,  2184,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1005,  1996,  8305,  1005,  2001,  1037,  6919,  3185,  2055,\n",
       "          1996,  2117,  9592,  2166,  4324,  2005,  2149,  1998,  2036,  8509,\n",
       "          2019,  6832,  2245,  2058,  1996,  4378,  1010,  2437,  2068,  5382,\n",
       "          2008,  2115,  5544,  2064,  2272,  2995,  1012,  2065,  2017,  3866,\n",
       "          1005,  3342,  1996, 13785,  1005,  1010,  1005,  1996,  8305,  1005,\n",
       "          2003,  1996,  3185,  2005,  2017,   999,   999,  2009,  1005,  1055,\n",
       "          1996,  2514,  2204,  3185,  1997,  1996,  2095,  1998,  2009,  2003,\n",
       "          1996,  3819,  3185,  2005,  2035,  5535,  1012,  1005,  1996,  8305,\n",
       "          1005,  4978,  1037,  2350,  2188,  2448,   999,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  7929,  1010,  2021,  2515,  2008,  2191,  2023,  1037,  2204,\n",
       "          3185,  1029,  2092,  1010,  2025,  2428,  1010,  1999,  2026,  5448,\n",
       "          1012,  2045,  3475,  1005,  1056,  1037,  2878,  2843,  2000, 16755,\n",
       "          2009,  1012,  1045,  2179,  2009,  2200,  4030,  1010,  6945, 19426,\n",
       "          1010,  1999,  2755,  1012,  2009,  1005,  1055,  2036, 21425,  3492,\n",
       "          2172,  2083,  1998,  2083,  1012,  2193,  2028,  1998,  2048,  2020,\n",
       "          5399, 21425,  1010,  2021,  2025,  2004,  2172,  1012,  1045,  2036,\n",
       "          2371,  2023,  3185,  2001,  3243,  3409,  2100,  2012,  2335,  1010,\n",
       "          2029,  1045,  2134,  1005,  1056,  2428,  2228, 16142,  2023,  2186,\n",
       "          1998,  1996,  2839,  1012,  5076,  6904, 14844,  3248,  1996,  2364,\n",
       "          2919,  3124,  1999,  2023, 18932,  1012,  2002,  1005,  1055,  1037,\n",
       "         11519,  2438,  3364,  1010,  2021,  1045,  2371,  2002,  2209,  2010,\n",
       "          2839,  2205,  2058,  1996,  2327,  1012,  1045,  3984,  2008,  4906,\n",
       "          2007,  1996,  4309,  1997,  1996,  3185,  1010,  2029,  2052,  2031,\n",
       "          2042,  2307,  2065,  1045,  2018,  4669,  1996,  3185,  1012,  4606,\n",
       "          1010,  2045,  2020,  2070,  3492,  2919,  2028, 11197,  2015,  1012,\n",
       "          7779, 29536, 14540,  9541,  5651,  1999,  1996,  2516,  2535,  1010,\n",
       "          2021,  2003,  2445,  2210,  2000,  2147,  2007,  1999,  2023,  3185,\n",
       "          1012,  1996,  2839,  2038,  2025,  2428,  7964,  1010,  2004,  1045,\n",
       "          2018,  5113,  1012,  2821,  2092,  1012,  2023,  2003,  2074,  2026,\n",
       "          5448,  1012,  4312,  1010,  2005,  2033,  1010,  2096,  2023,  3185,\n",
       "          2003,  2025, 11113,  7274,  9067,  1010,  2009,  2003,  3492,  2919,\n",
       "          1012,  2026,  3789,  2005,  2601,  2386,  3523,  1024,  1017,  1012,\n",
       "          1019,  1013,  1019,   102]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_encoding[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_encoding[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2754, 17241,  2411,  2031,  1037,  2350,  6346,  1012,  2027,  2411,\n",
       "          2272,  2041,  2559,  2066,  1037,  2143,  4950,  2001,  3432,  2872,\n",
       "          2006,  1996,  2754,  1006,  2107,  2004,  1000,  2305,  2388,  1000,\n",
       "          1007,  1012, 11430, 11320, 11368,  1005,  1055,  3257,  7906,  1996,\n",
       "          2143,  4142,  1010,  2029,  2003,  2926,  3697,  2144,  1996,  3861,\n",
       "          3253,  2032,  2053,  2613,  4119,  1012,  2145,  1010,  2009,  1005,\n",
       "          1055,  3835,  2000,  2298,  2012,  2005,  2054,  2009,  2003,  1012,\n",
       "          1996,  6370,  2090,  2745, 19881,  1998,  5696, 20726,  2003,  3243,\n",
       "          8235,  1012,  1996, 10949,  1997,  2037,  3276,  2024, 11341,  1012,\n",
       "         19881,  2003, 10392,  2004,  2467,  1010,  1998, 20726,  4152,  2028,\n",
       "          1997,  2010,  2261,  9592,  2000,  2428,  2552,  1012,  1026,  7987,\n",
       "          1013,  1028,  1026,  7987,  1013,  1028,  1045, 18766,  2008,  1045,\n",
       "          1005,  2310,  2196,  2464, 11209, 20206,  1005,  1055,  2377,  1010,\n",
       "          2021,  1045,  2963,  2008,  6108,  2811,  2239,  5297,  1005,  1055,\n",
       "          6789,  2003, 11633,  1012,  1996,  5896,  2003, 11757,  9530,  6767,\n",
       "          7630,  3064,  1010,  1998,  7906,  2017, 16986,  1012,  1000,  2331,\n",
       "          6494,  2361,  1000,  2003,  2019,  8216,  2135, 14036,  2143,  1010,\n",
       "          1998,  2003,  6749,  2005,  3053,  2035,  4599,  1997,  2754,  1998,\n",
       "          3898,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,\n",
       "          1021,  1012,  1018,  2041,  1997,  2184,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [ 1005,  1996,  8305,  1005,  2001,  1037,  6919,  3185,  2055,  1996,\n",
       "          2117,  9592,  2166,  4324,  2005,  2149,  1998,  2036,  8509,  2019,\n",
       "          6832,  2245,  2058,  1996,  4378,  1010,  2437,  2068,  5382,  2008,\n",
       "          2115,  5544,  2064,  2272,  2995,  1012,  2065,  2017,  3866,  1005,\n",
       "          3342,  1996, 13785,  1005,  1010,  1005,  1996,  8305,  1005,  2003,\n",
       "          1996,  3185,  2005,  2017,   999,   999,  2009,  1005,  1055,  1996,\n",
       "          2514,  2204,  3185,  1997,  1996,  2095,  1998,  2009,  2003,  1996,\n",
       "          3819,  3185,  2005,  2035,  5535,  1012,  1005,  1996,  8305,  1005,\n",
       "          4978,  1037,  2350,  2188,  2448,   999,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [ 7929,  1010,  2021,  2515,  2008,  2191,  2023,  1037,  2204,  3185,\n",
       "          1029,  2092,  1010,  2025,  2428,  1010,  1999,  2026,  5448,  1012,\n",
       "          2045,  3475,  1005,  1056,  1037,  2878,  2843,  2000, 16755,  2009,\n",
       "          1012,  1045,  2179,  2009,  2200,  4030,  1010,  6945, 19426,  1010,\n",
       "          1999,  2755,  1012,  2009,  1005,  1055,  2036, 21425,  3492,  2172,\n",
       "          2083,  1998,  2083,  1012,  2193,  2028,  1998,  2048,  2020,  5399,\n",
       "         21425,  1010,  2021,  2025,  2004,  2172,  1012,  1045,  2036,  2371,\n",
       "          2023,  3185,  2001,  3243,  3409,  2100,  2012,  2335,  1010,  2029,\n",
       "          1045,  2134,  1005,  1056,  2428,  2228, 16142,  2023,  2186,  1998,\n",
       "          1996,  2839,  1012,  5076,  6904, 14844,  3248,  1996,  2364,  2919,\n",
       "          3124,  1999,  2023, 18932,  1012,  2002,  1005,  1055,  1037, 11519,\n",
       "          2438,  3364,  1010,  2021,  1045,  2371,  2002,  2209,  2010,  2839,\n",
       "          2205,  2058,  1996,  2327,  1012,  1045,  3984,  2008,  4906,  2007,\n",
       "          1996,  4309,  1997,  1996,  3185,  1010,  2029,  2052,  2031,  2042,\n",
       "          2307,  2065,  1045,  2018,  4669,  1996,  3185,  1012,  4606,  1010,\n",
       "          2045,  2020,  2070,  3492,  2919,  2028, 11197,  2015,  1012,  7779,\n",
       "         29536, 14540,  9541,  5651,  1999,  1996,  2516,  2535,  1010,  2021,\n",
       "          2003,  2445,  2210,  2000,  2147,  2007,  1999,  2023,  3185,  1012,\n",
       "          1996,  2839,  2038,  2025,  2428,  7964,  1010,  2004,  1045,  2018,\n",
       "          5113,  1012,  2821,  2092,  1012,  2023,  2003,  2074,  2026,  5448,\n",
       "          1012,  4312,  1010,  2005,  2033,  1010,  2096,  2023,  3185,  2003,\n",
       "          2025, 11113,  7274,  9067,  1010,  2009,  2003,  3492,  2919,  1012,\n",
       "          2026,  3789,  2005,  2601,  2386,  3523,  1024,  1017,  1012,  1019,\n",
       "          1013,  1019]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – shows how to drop the special tokens\n",
    "bert_encoding = bert_tokenizer(train_reviews[:3], padding=True,\n",
    "                               truncation=True, max_length=500,\n",
    "                               add_special_tokens=False, return_tensors=\"pt\")\n",
    "bert_encoding[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram LM is used by models like ALBERT, T5, and XLM-R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  876,  5004,    18,   478,    57,    21,   394,  4173,     9,    59,\n",
       "           478,   340,    70,   699,   101,    21,   171,  3336,    23,  1659,\n",
       "          1037,    27,    14,   876,    13,     5,  4289,    28,    13,     7,\n",
       "          4893,   449,     7,     6,     9, 12508,  1612,  5909,    22,    18,\n",
       "          1400,  8968,    14,   171,  2481,    15,    56,    25,  1118,  1956,\n",
       "           179,    14,  2151,  1434,    61,    90,   683,  2404,     9,   174,\n",
       "            15,    32,    22,    18,  2210,    20,   361,    35,    26,    98,\n",
       "            32,    25,     9,    14,  5427,   128,   832, 22427,    17,  4479,\n",
       "         24604,    25,  1450,  7472,     9,    14, 12289,    16,    66,  1429,\n",
       "            50, 12891,     9, 22427,    25, 10356,    28,   550,    15,    17,\n",
       "         24604,  3049,    53,    16,    33,   310, 11285,    20,   510,   601,\n",
       "             9,     1,  5145,    13,   118,     1,  5145,    13,   118,     1,\n",
       "            49, 14586,    30,    31,    22,   195,   243,   541, 16216, 18863,\n",
       "            22,    18,   418,    15,    47,    31,   990,    30,  3361,   901,\n",
       "           218,  3675,    22,    18,  5004,    25, 10763,     9,    14,  3884,\n",
       "            25, 13003,  1065, 16261,  1427,    15,    17,  8968,    42, 19523,\n",
       "             9,    13,     7, 13921, 16514,     7,    25,    40,  7135,   102,\n",
       "         15677,   171,    15,    17,    25,  5773,    26,  1212,    65,  3047,\n",
       "            16,   876,    17,  2324,     9,     1,  5145,    13,   118,     1,\n",
       "          5145,    13,   118,     1,   465,     9,   300,    70,    16,   332,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [   13,    22,   124,  7959,    22,    23,    21,  5934,  1308,    88,\n",
       "            14,   153, 11285,   201,  2763,    26,   182,    17,    67, 11179,\n",
       "            40,  6090,   289,    84,    14,  2663,    15,   544,   105,  4007,\n",
       "            30,   154,  4412,    92,   340,  1151,     9,   100,    42,  2199,\n",
       "            13,    22, 18342,    14, 15771,    22,    15,    13,    22,   124,\n",
       "          7959,    22,    25,    14,  1308,    26,    42, 19015,    32,    22,\n",
       "            18,    14,   583,   254,  1308,    16,    14,   159,    17,    32,\n",
       "            25,    14,  2107,  1308,    26,    65,  5520,     9,    13,    22,\n",
       "           124,  7959,    22,  3858,    21,   394,   213,   485,   187,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [ 5854,    15,   811,   630,    30,   233,    48,    21,   254,  1308,\n",
       "            60,   854,    15,  1270,   510,    15,   108,    51,  4052,     9,\n",
       "          1887,  2532,    22,    38,    21,   979,   865,    20, 12360,    32,\n",
       "             9,    49,   216,    32,   253,  2276,    15,  1427, 18190,    15,\n",
       "           108,   837,     9,   242,    22,    18,    67, 24109,  1772,   212,\n",
       "           120,    17,   120,     9, 16299,    53,    17,    81,    46,  4131,\n",
       "         24109,    15,   811,    52,    28,   212,     9,    49,    67,   427,\n",
       "            48,  1308,    23,  1450,  1232,    93,    35,   436,    15,  2140,\n",
       "            31,   223,    22,    38,   510,   277,  2742,    18,    48,   231,\n",
       "            17,    14,   925,     9, 27048,  1399,  3909,  1533,    14,   407,\n",
       "           896,  1244,    19,    48, 18008,     9,   438,    22,    18,    21,\n",
       "         12238,   511,  1574,    15,   811,    31,   427,    24,   257,    33,\n",
       "           925,   266,    84,    14,   371,     9,    49,  2321,    30,  2742,\n",
       "            29,    14,  2919,    16,    14,  1308,    15,  2140,    83,    57,\n",
       "            74,   374,   100,    31,    41,  3345,    14,  1308,     9, 13349,\n",
       "            15,  1887,    46,   109,  1772,   896,    53, 12588,    18,     9,\n",
       "         10818,  1218, 13547, 10165,  4815,    19,    14,   581,   597,    15,\n",
       "           811,    25,   504,   265,    20,   170,    29,    19,    48,  1308,\n",
       "             9,   124,   925,    63,    52,   510,  7339,    15,   472,    31,\n",
       "            41,  3691,     9,  1134,   134,     9,  1565,    25,   114,    51,\n",
       "          4052,     9, 23304,    15,  1106,    55,    15, 10144,    48,  1308,\n",
       "            25,    52,    21,   779,    18,  3482,    15,   242,    25,  1772,\n",
       "           896,     9,   915,  2018,    26,   659,   177,  1867,    45,   203,\n",
       "             9, 10551,   264]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "albert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "albert_encoding = albert_tokenizer(\n",
    "    train_reviews[:3], padding=True, truncation=True, max_length=500,\n",
    "    add_special_tokens=False, return_tensors=\"pt\")\n",
    "albert_token_ids = albert_encoding[\"input_ids\"]\n",
    "albert_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[159, 402, 176, 246,  61, 782, 156, 737, 252,  42, 239,  51, 154, 460,\n",
       "         917,  17, 272, 156, 737, 576, 215, 976, 275,  42, 199,  44, 554,  42,\n",
       "         192, 585,  57, 160, 259, 170, 157, 143, 138, 159, 402,  11, 589, 152,\n",
       "           5, 819, 168, 230,   5, 521, 924, 981, 962, 250,  61,  10,  60, 426,\n",
       "         526, 959,  60, 138, 199, 150, 319,  15, 363, 141, 957, 694,  47, 696,\n",
       "          61, 875, 138, 960, 337, 414, 140, 157, 385, 174, 433, 161, 221, 145,\n",
       "         213,  17, 549,  15, 151,  10,  60,  55, 416, 146, 407, 144, 182, 303,\n",
       "         151, 141,  17, 138, 547, 538, 528, 768,  54, 335,  42, 203,  44, 270,\n",
       "          46, 153, 876, 141, 919, 233, 522, 172, 141, 719, 162, 807, 279,  17,\n",
       "         138,  45,  66,  55, 188, 989, 156, 378, 698, 301, 296, 689, 212, 558,\n",
       "         926, 148,  17,  44, 270,  46, 141,  47, 279, 302, 171, 152, 787,  15,\n",
       "         153, 522, 172, 766, 205, 156, 234, 677, 161, 139, 513, 146, 370, 251,\n",
       "         219, 162, 197, 162, 166,  50, 265,  47, 266, 177,  50,  10, 172, 502,\n",
       "         499, 210,  42, 163,  63, 137,  10,  60, 387,  15, 209,  50, 183, 155,\n",
       "         177,  51, 186, 774, 143, 221, 145,  10,  60, 176, 246,  61, 301, 141,\n",
       "         460,  50, 136, 355,  17, 138, 778, 141, 137, 534, 271,  43, 160, 265,\n",
       "          63, 290, 179, 157,  15, 153, 959,  60, 206, 360, 266, 148,  17,   5,\n",
       "         222, 606, 241, 246,   5, 141, 139, 145, 154,  54, 287, 160, 885, 148,\n",
       "         199,  15, 153, 141, 142, 994, 157, 182, 236, 637, 221,  47, 489, 156,\n",
       "         159, 402, 153, 718, 219, 162, 197, 162, 166,  26,  17,  23, 215, 156,\n",
       "         586,   0,   0,   0,   0],\n",
       "        [ 10, 138, 198, 289, 175,  10, 192,  42, 725, 355, 211, 311, 138, 964,\n",
       "         161, 139, 513, 493, 204, 207,  60, 182, 244, 153, 434, 679,  60, 139,\n",
       "          46, 854, 904, 733, 394, 138, 870, 415,  15, 839, 419, 433, 544,  46,\n",
       "         177, 508,  45, 142, 188,  60, 313, 576, 986,  17, 249, 206, 180, 541,\n",
       "          10, 142, 984, 138, 899, 489,  10,  15,  10, 138, 198, 289, 175,  10,\n",
       "         141, 138, 211, 182, 206, 584, 151,  10,  60, 138, 580, 321, 211, 156,\n",
       "         138, 967, 153, 151, 141, 138, 257, 684, 211, 182, 221, 281, 149,  17,\n",
       "          10, 138, 198, 289, 175,  10,  49, 377,  42, 239,  51, 154,  49, 240,\n",
       "         841,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0],\n",
       "        [289,  15, 209, 398, 177, 442, 173,  42, 321, 211,  34, 371,  15, 226,\n",
       "         370,  15, 137, 306, 583, 137, 185,  17, 291, 809,  10,  61,  42, 833,\n",
       "         384, 146, 142, 994, 151,  17,  50, 920, 151, 268,  60, 413,  15, 712,\n",
       "         399, 160,  15, 137, 727,  17, 151,  10,  60, 434, 295,  45, 543, 344,\n",
       "         735, 417, 525, 153, 525,  17,  55, 357, 440, 205, 153, 495, 382, 253,\n",
       "         303, 295,  45, 543, 344,  15, 209, 226, 152, 417,  17,  50, 434,  47,\n",
       "         203,  61, 173, 211, 192, 719,  44, 188,  57,  66, 144, 650,  15, 363,\n",
       "          50, 653,  10,  61, 370, 439,  47, 377, 173, 761, 153, 138, 331,  17,\n",
       "          51, 553,  47, 460, 183,  66, 387,  60, 138, 783, 404, 652, 137, 173,\n",
       "         137, 159, 221, 323,  17, 183,  10,  60,  42, 222, 530, 759, 477,  15,\n",
       "         209,  50,  47, 203,  61, 183, 910, 234, 331, 450, 394, 138, 919,  17,\n",
       "          50, 360, 266, 177,  47, 151, 201, 138,  61, 205, 156, 138, 211,  15,\n",
       "         363, 343, 252, 428, 403, 249,  50, 369, 190, 930, 138, 211,  17, 259,\n",
       "         244,  15, 291, 382, 253, 735, 404, 205, 849, 315,  17, 155, 174, 207,\n",
       "         838,  60, 180,  56, 142, 617,  60, 137, 138, 899, 163, 806,  15, 209,\n",
       "         141, 518, 293, 520, 146, 455, 201, 137, 173, 211,  17, 138, 331, 308,\n",
       "         226, 370, 598, 290, 541,  15, 152,  50, 369, 204, 938,  17,  56,  49,\n",
       "         371,  17, 173, 141, 299, 306, 583, 137, 185,  17, 317, 307,  15, 182,\n",
       "         250,  15, 548, 173, 211, 141, 226, 202,  66, 685, 150,  15, 151, 141,\n",
       "         735, 404,  17, 306, 838, 286, 182,  45, 900, 243,  50,  50,  50,  29,\n",
       "          22,  17,  24,  18,  24]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer = transformers.PreTrainedTokenizerFast(\n",
    "    tokenizer_object=bpe_tokenizer)\n",
    "hf_encodings = hf_tokenizer(train_reviews[:3], padding=True, truncation=True,\n",
    "                            max_length=500, return_tensors=\"pt\")\n",
    "hf_encodings[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training a Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer=bert_tokenizer):\n",
    "    reviews = [review[\"text\"] for review in batch]\n",
    "    labels = [[review[\"label\"]] for review in batch]\n",
    "    encodings = tokenizer(reviews, padding=True, truncation=True,\n",
    "                          max_length=200, return_tensors=\"pt\")\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return encodings, labels\n",
    "\n",
    "batch_size = 256\n",
    "imdb_train_loader = DataLoader(imdb_train_set, batch_size=batch_size,\n",
    "                               collate_fn=collate_fn, shuffle=True)\n",
    "imdb_valid_loader = DataLoader(imdb_valid_set, batch_size=batch_size,\n",
    "                               collate_fn=collate_fn)\n",
    "imdb_test_loader = DataLoader(imdb_test_set, batch_size=batch_size,\n",
    "                              collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers=2, embedding_size=128,\n",
    "                 hidden_size=64, pad_id=0, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size,\n",
    "                                  padding_idx=pad_id)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        embeddings = self.embed(encodings[\"input_ids\"])\n",
    "        _outputs, hidden_states = self.gru(embeddings)\n",
    "        return self.output(hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([5, 1, 6, 2, 7, 8]), batch_sizes=tensor([2, 2, 1, 1]), sorted_indices=tensor([1, 0]), unsorted_indices=tensor([1, 0]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "sequences = torch.tensor([[1, 2, 0, 0], [5, 6, 7, 8]])\n",
    "packed = pack_padded_sequence(sequences, lengths=(2, 4),\n",
    "                              enforce_sorted=False, batch_first=True)\n",
    "packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2, 0, 0],\n",
       "         [5, 6, 7, 8]]),\n",
       " tensor([2, 4]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded, lengths = pad_packed_sequence(packed, batch_first=True)\n",
    "padded, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModelPackedSeq(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers=2, embedding_size=128,\n",
    "                 hidden_size=64, pad_id=0, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size,\n",
    "                                  padding_idx=pad_id)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        embeddings = self.embed(encodings[\"input_ids\"])\n",
    "        lengths = encodings[\"attention_mask\"].sum(dim=1)                      # <= line added\n",
    "        packed = pack_padded_sequence(embeddings, lengths=lengths.cpu(),      # <= line added\n",
    "                                      batch_first=True, enforce_sorted=False) # <= line added\n",
    "        _outputs, hidden_states = self.gru(packed)                            # <= line changed\n",
    "        return self.output(hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10,                      train loss: 0.6640, train metric: 59.20%, valid metric: 60.30%\n",
      "Epoch 2/10,                      train loss: 0.5749, train metric: 70.63%, valid metric: 65.98%\n",
      "Epoch 3/10,                      train loss: 0.4963, train metric: 76.87%, valid metric: 65.38%\n",
      "Epoch 4/10,                      train loss: 0.3539, train metric: 84.68%, valid metric: 82.78%\n",
      "Epoch 5/10,                      train loss: 0.2662, train metric: 89.26%, valid metric: 83.76%\n",
      "Epoch 6/10,                      train loss: 0.1985, train metric: 92.58%, valid metric: 85.14%\n",
      "Epoch 7/10,                      train loss: 0.1528, train metric: 94.55%, valid metric: 85.54%\n",
      "Epoch 8/10,                      train loss: 0.1168, train metric: 96.21%, valid metric: 85.20%\n",
      "Epoch 9/10,                      train loss: 0.0637, train metric: 98.22%, valid metric: 83.12%\n",
      "Epoch 10/10,                      train loss: 0.0672, train metric: 98.37%, valid metric: 81.68%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "vocab_size = bert_tokenizer.vocab_size\n",
    "imdb_model_ps = SentimentAnalysisModelPackedSeq(vocab_size).to(device)\n",
    "\n",
    "n_epochs = 10\n",
    "xentropy = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(imdb_model_ps.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "history = train(imdb_model_ps, optimizer, xentropy, accuracy,\n",
    "                imdb_train_loader, imdb_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModelBidi(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers=2, embedding_size=128,\n",
    "                 hidden_size=64, pad_id=0, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size,\n",
    "                                  padding_idx=pad_id)\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout, bidirectional=True)  # <= line changed\n",
    "        self.output = nn.Linear(2 * hidden_size, 1)                               # <= line changed\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        embeddings = self.embed(encodings[\"input_ids\"])\n",
    "        lengths = encodings[\"attention_mask\"].sum(dim=1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths=lengths.cpu(),\n",
    "                                      batch_first=True, enforce_sorted=False)\n",
    "        _outputs, hidden_states = self.gru(packed)\n",
    "        n_dims = self.output.in_features                                          # <= line added\n",
    "        top_states = hidden_states[-2:].permute(1, 0, 2).reshape(-1, n_dims)      # <= line added\n",
    "        return self.output(top_states)                                            # <= line changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10,                      train loss: 0.6528, train metric: 60.66%, valid metric: 72.78%\n",
      "Epoch 2/10,                      train loss: 0.5046, train metric: 75.52%, valid metric: 77.32%\n",
      "Epoch 3/10,                      train loss: 0.3865, train metric: 82.78%, valid metric: 80.82%\n",
      "Epoch 4/10,                      train loss: 0.2942, train metric: 87.85%, valid metric: 81.86%\n",
      "Epoch 5/10,                      train loss: 0.2445, train metric: 90.20%, valid metric: 73.08%\n",
      "Epoch 6/10,                      train loss: 0.1561, train metric: 94.13%, valid metric: 83.76%\n",
      "Epoch 7/10,                      train loss: 0.1020, train metric: 96.36%, valid metric: 83.76%\n",
      "Epoch 8/10,                      train loss: 0.0496, train metric: 98.42%, valid metric: 79.04%\n",
      "Epoch 9/10,                      train loss: 0.0714, train metric: 97.77%, valid metric: 82.76%\n",
      "Epoch 10/10,                      train loss: 0.0180, train metric: 99.66%, valid metric: 82.54%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "vocab_size = bert_tokenizer.vocab_size\n",
    "imdb_model_bidi = SentimentAnalysisModelBidi(vocab_size).to(device)\n",
    "\n",
    "n_epochs = 10\n",
    "xentropy = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(imdb_model_bidi.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "history = train(imdb_model_bidi, optimizer, xentropy, accuracy, imdb_train_loader,\n",
    "                imdb_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Embeddings and Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModelPreEmbeds(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, n_layers=2, hidden_size=64,\n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "        weights = pretrained_embeddings.weight.data\n",
    "        self.embed = nn.Embedding.from_pretrained(weights, freeze=True)\n",
    "        embedding_size = weights.shape[-1]\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.output = nn.Linear(2 * hidden_size, 1)\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        embeddings = self.embed(encodings[\"input_ids\"])\n",
    "        lengths = encodings[\"attention_mask\"].sum(dim=1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths=lengths.cpu(),\n",
    "                                      batch_first=True, enforce_sorted=False)\n",
    "        _outputs, hidden_states = self.gru(packed)\n",
    "        n_dims = self.output.in_features\n",
    "        top_states = hidden_states[-2:].permute(1, 0, 2).reshape(-1, n_dims)\n",
    "        return self.output(top_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10,                      train loss: 0.6918, train metric: 55.10%, valid metric: 52.20%\n",
      "Epoch 2/10,                      train loss: 0.6499, train metric: 63.81%, valid metric: 71.42%\n",
      "Epoch 3/10,                      train loss: 0.5545, train metric: 72.01%, valid metric: 72.48%\n",
      "Epoch 4/10,                      train loss: 0.4817, train metric: 77.03%, valid metric: 78.92%\n",
      "Epoch 5/10,                      train loss: 0.4143, train metric: 80.99%, valid metric: 78.60%\n",
      "Epoch 6/10,                      train loss: 0.3678, train metric: 83.38%, valid metric: 83.78%\n",
      "Epoch 7/10,                      train loss: 0.3337, train metric: 85.26%, valid metric: 83.62%\n",
      "Epoch 8/10,                      train loss: 0.3050, train metric: 86.92%, valid metric: 82.52%\n",
      "Epoch 9/10,                      train loss: 0.2856, train metric: 87.84%, valid metric: 68.18%\n",
      "Epoch 10/10,                      train loss: 0.2435, train metric: 90.06%, valid metric: 82.28%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "imdb_model_bert_embeds = SentimentAnalysisModelPreEmbeds(\n",
    "    bert_model.embeddings.word_embeddings).to(device)\n",
    "\n",
    "n_epochs = 10\n",
    "xentropy = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(imdb_model_bert_embeds.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "history = train(imdb_model_bert_embeds, optimizer, xentropy, accuracy,\n",
    "                imdb_train_loader, imdb_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 200, 768])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_encoding = bert_tokenizer(train_reviews[:3], padding=True,\n",
    "                               max_length=200, truncation=True,\n",
    "                               return_tensors=\"pt\")\n",
    "bert_output = bert_model(**bert_encoding)\n",
    "bert_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_output.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModelBert(nn.Module):\n",
    "    def __init__(self, n_layers=2, hidden_size=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.bert = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        embedding_size = self.bert.config.hidden_size\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers=n_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        contextualized_embeddings = self.bert(**encodings).last_hidden_state\n",
    "        lengths = encodings[\"attention_mask\"].sum(dim=1)\n",
    "        packed = pack_padded_sequence(contextualized_embeddings, lengths=lengths.cpu(),\n",
    "                                      batch_first=True, enforce_sorted=False)\n",
    "        _outputs, hidden_states = self.gru(packed)\n",
    "        return self.output(hidden_states[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4,                      train loss: 0.4860, train metric: 75.67%, valid metric: 87.02%\n",
      "Epoch 2/4,                      train loss: 0.3024, train metric: 87.01%, valid metric: 88.80%\n",
      "Epoch 3/4,                      train loss: 0.2728, train metric: 88.68%, valid metric: 87.60%\n",
      "Epoch 4/4,                      train loss: 0.2531, train metric: 89.46%, valid metric: 86.68%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "imdb_model_bert = SentimentAnalysisModelBert().to(device)\n",
    "imdb_model_bert.bert.requires_grad_(False)\n",
    "\n",
    "n_epochs = 4\n",
    "xentropy = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(imdb_model_bert.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "history = train(imdb_model_bert, optimizer, xentropy, accuracy,\n",
    "                imdb_train_loader, imdb_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModelBert2(nn.Module):\n",
    "    def __init__(self, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.bert = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.output = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        bert_output = self.bert(**encodings)\n",
    "        return self.output(bert_output.last_hidden_state[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModelBert3(nn.Module):\n",
    "    def __init__(self, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.bert = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.output = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, encodings):\n",
    "        bert_output = self.bert(**encodings)\n",
    "        return self.output(bert_output.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5,                      train loss: 0.6593, train metric: 61.35%, valid metric: 67.72%\n",
      "Epoch 2/5,                      train loss: 0.6035, train metric: 69.63%, valid metric: 68.34%\n",
      "Epoch 3/5,                      train loss: 0.5710, train metric: 72.01%, valid metric: 74.64%\n",
      "Epoch 4/5,                      train loss: 0.5540, train metric: 73.47%, valid metric: 71.56%\n",
      "Epoch 5/5,                      train loss: 0.5365, train metric: 74.22%, valid metric: 59.56%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "imdb_model_bert3 = SentimentAnalysisModelBert3().to(device)\n",
    "imdb_model_bert3.bert.requires_grad_(False)\n",
    "\n",
    "n_epochs = 5\n",
    "xentropy = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.NAdam(imdb_model_bert3.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "history = train(imdb_model_bert3, optimizer, xentropy, accuracy,\n",
    "                imdb_train_loader, imdb_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5,                      train loss: 0.8118, train metric: 71.17%, valid metric: 81.50%\n",
      "Epoch 2/5,                      train loss: 0.4543, train metric: 79.32%, valid metric: 81.00%\n",
      "Epoch 3/5,                      train loss: 0.4418, train metric: 79.50%, valid metric: 80.36%\n",
      "Epoch 4/5,                      train loss: 0.4192, train metric: 80.70%, valid metric: 77.80%\n",
      "Epoch 5/5,                      train loss: 0.4038, train metric: 81.81%, valid metric: 82.68%\n"
     ]
    }
   ],
   "source": [
    "imdb_model_bert3.bert.pooler.requires_grad_(True)\n",
    "\n",
    "history = train(imdb_model_bert3, optimizer, xentropy, accuracy,\n",
    "                imdb_train_loader, imdb_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-Specific Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "torch.manual_seed(42)\n",
    "bert_for_binary_clf = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2468, 0.3499]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = bert_tokenizer([\"This was a great movie!\"])\n",
    "output = bert_for_binary_clf(\n",
    "    input_ids=torch.tensor(encoding[\"input_ids\"]),\n",
    "    attention_mask=torch.tensor(encoding[\"attention_mask\"]))\n",
    "\n",
    "output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4743, 0.5257]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(output.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6429, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = bert_for_binary_clf(\n",
    "    input_ids=torch.tensor(encoding[\"input_ids\"]),\n",
    "    attention_mask=torch.tensor(encoding[\"attention_mask\"]),\n",
    "    labels=torch.tensor([1]))\n",
    "\n",
    "output.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every model from the Transformers library has a `config` attribute that contains the model's configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"problem_type\": \"single_label_classification\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_for_binary_clf.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use our own training function to train this model, or we can use the Trainer API instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Trainer API needs datasets that are already tokenized, so let's prepare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46713a7a68b45e79f67d1608d128d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3e68cd9eac41a98b2c8c9e01434628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934236fc627e4704bc1e04e19cd04d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_batch(batch):\n",
    "    return bert_tokenizer(batch[\"text\"], truncation=True, max_length=200)\n",
    "\n",
    "tok_imdb_train_set = imdb_train_set.map(tokenize_batch, batched=True)\n",
    "tok_imdb_valid_set = imdb_valid_set.map(tokenize_batch, batched=True)\n",
    "tok_imdb_test_set = imdb_test_set.map(tokenize_batch, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Trainer API doesn't support streaming metrics so we need to write our own evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(pred):\n",
    "    return {\"accuracy\": (pred.label_ids == pred.predictions.argmax(-1)).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"my_imdb_model\", num_train_epochs=2,\n",
    "    per_device_train_batch_size=128, per_device_eval_batch_size=128,\n",
    "    eval_strategy=\"epoch\", logging_strategy=\"epoch\", save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True, metric_for_best_model=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='314' max='314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [314/314 12:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.241851</td>\n",
       "      <td>0.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.161500</td>\n",
       "      <td>0.242788</td>\n",
       "      <td>0.907000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    bert_for_binary_clf, train_args, train_dataset=tok_imdb_train_set,\n",
    "    eval_dataset=tok_imdb_valid_set, compute_metrics=compute_accuracy,\n",
    "    data_collator=DataCollatorWithPadding(bert_tokenizer))\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996108412742615},\n",
       " {'label': 'POSITIVE', 'score': 0.9998623132705688},\n",
       " {'label': 'NEGATIVE', 'score': 0.9943684935569763},\n",
       " {'label': 'POSITIVE', 'score': 0.997913658618927},\n",
       " {'label': 'POSITIVE', 'score': 0.999544084072113},\n",
       " {'label': 'NEGATIVE', 'score': 0.9845332503318787},\n",
       " {'label': 'POSITIVE', 'score': 0.9859278202056885},\n",
       " {'label': 'POSITIVE', 'score': 0.9993758797645569},\n",
       " {'label': 'POSITIVE', 'score': 0.9978922009468079},\n",
       " {'label': 'NEGATIVE', 'score': 0.9997020363807678}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "classifier_imdb = pipeline(\"sentiment-analysis\", model=model_name,\n",
    "                           truncation=True, max_length=512)\n",
    "classifier_imdb(train_reviews[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8820, device='mps:0')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "with torch.no_grad():\n",
    "    text_imdb_valid_loader = DataLoader(imdb_valid_set, batch_size=256)\n",
    "    for index, batch in enumerate(text_imdb_valid_loader):\n",
    "        y_pred = classifier_imdb(batch[\"text\"], truncation=True)\n",
    "        y_pred = torch.tensor([pred[\"label\"] == \"POSITIVE\" for pred in y_pred], dtype=int)\n",
    "        accuracy.update(y_pred, batch[\"label\"])\n",
    "        print(f\"\\r{index + 1}/{len(text_imdb_valid_loader)}\", end=\"\")\n",
    "\n",
    "accuracy.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be very biased. For example, it may like or dislike some countries depending on the data it was trained on, and how it is used, so use it with care:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Iraq', {'label': 'NEGATIVE', 'score': 0.9706069231033325}),\n",
       " ('Thailand', {'label': 'POSITIVE', 'score': 0.9903932213783264}),\n",
       " ('the USA', {'label': 'POSITIVE', 'score': 0.9642282128334045}),\n",
       " ('Vietnam', {'label': 'NEGATIVE', 'score': 0.9747399091720581})]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – shows that binary classification can amplify model bias\n",
    "countries = [\"Iraq\", \"Thailand\", \"the USA\", \"Vietnam\"]\n",
    "texts = [f\"I am from {country}\" for country in countries]\n",
    "list(zip(countries, classifier_imdb(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Iraq', {'label': 'neutral', 'score': 0.8353288769721985}),\n",
       " ('Thailand', {'label': 'neutral', 'score': 0.8824350237846375}),\n",
       " ('the USA', {'label': 'neutral', 'score': 0.8349123597145081}),\n",
       " ('Vietnam', {'label': 'neutral', 'score': 0.8436853885650635})]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – using a model with a neutral class solves this bias issue\n",
    "# note: the warning is normal: this model's pooler will not be used for\n",
    "# classification, so its weights are downloaded but not used.\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "classifier_imdb_with_neutral = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "list(zip(countries, classifier_imdb_with_neutral(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'contradiction', 'score': 0.9717152714729309},\n",
       " {'label': 'entailment', 'score': 0.9119168519973755},\n",
       " {'label': 'neutral', 'score': 0.9509280323982239}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
    "classifier_mnli = pipeline(\"text-classification\", model=model_name)\n",
    "classifier_mnli([\n",
    "    \"She loves me. [SEP] She loves me not. [SEP]\",\n",
    "    \"Alice just woke up. [SEP] Alice is awake. [SEP]\",\n",
    "    \"I like dogs. [SEP] Everyone likes dogs. [SEP]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Encoder–Decoder Network for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_dataset = load_dataset(\"Helsinki-NLP/tatoeba_mt\", language_pair=\"eng-spa\",\n",
    "                           trust_remote_code=True)  # only if you trust the user\n",
    "split = nmt_dataset[\"validation\"].train_test_split(train_size=0.8, seed=42)\n",
    "nmt_train_set, nmt_valid_set = split[\"train\"], split[\"test\"]\n",
    "nmt_test_set = nmt_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sourceLang': 'eng',\n",
       " 'targetlang': 'spa',\n",
       " 'sourceString': \"Turn the light off. I can't fall asleep.\",\n",
       " 'targetString': 'Apaga la luz. No me puedo dormir.'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_eng_spa():  # a generator function to iterate over all training text\n",
    "    for pair in nmt_train_set:\n",
    "        yield pair[\"sourceString\"]\n",
    "        yield pair[\"targetString\"]\n",
    "\n",
    "max_length = 500\n",
    "vocab_size = 10_000\n",
    "\n",
    "nmt_tokenizer_model = tokenizers.models.BPE(unk_token=\"<unk>\")\n",
    "nmt_tokenizer = tokenizers.Tokenizer(nmt_tokenizer_model)\n",
    "nmt_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
    "nmt_tokenizer.enable_truncation(max_length=max_length)\n",
    "nmt_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "nmt_tokenizer_trainer = tokenizers.trainers.BpeTrainer(\n",
    "    vocab_size=vocab_size, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"])\n",
    "nmt_tokenizer.train_from_iterator(train_eng_spa(), nmt_tokenizer_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43, 403, 4300]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_tokenizer.encode(\"I like soccer\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 397, 583, 221, 3325]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_tokenizer.encode(\"<s> Me gusta el fútbol\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "fields = [\"src_token_ids\", \"src_mask\", \"tgt_token_ids\", \"tgt_mask\"]\n",
    "class NmtPair(namedtuple(\"NmtPairBase\", fields)):\n",
    "    def to(self, device):\n",
    "        return NmtPair(self.src_token_ids.to(device), self.src_mask.to(device), \n",
    "                       self.tgt_token_ids.to(device), self.tgt_mask.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmt_collate_fn(batch):\n",
    "    src_texts = [pair['sourceString'] for pair in batch]\n",
    "    tgt_texts = [f\"<s> {pair['targetString']} </s>\" for pair in batch]\n",
    "    src_encodings = nmt_tokenizer.encode_batch(src_texts)\n",
    "    tgt_encodings = nmt_tokenizer.encode_batch(tgt_texts)\n",
    "    src_token_ids = torch.tensor([enc.ids for enc in src_encodings])\n",
    "    tgt_token_ids = torch.tensor([enc.ids for enc in tgt_encodings])\n",
    "    src_mask = torch.tensor([enc.attention_mask for enc in src_encodings])\n",
    "    tgt_mask = torch.tensor([enc.attention_mask for enc in tgt_encodings])\n",
    "    inputs = NmtPair(src_token_ids, src_mask,\n",
    "                     tgt_token_ids[:, :-1], tgt_mask[:, :-1])\n",
    "    labels = tgt_token_ids[:, 1:]\n",
    "    return inputs, labels\n",
    "\n",
    "batch_size = 256\n",
    "nmt_train_loader = DataLoader(nmt_train_set, batch_size=batch_size,\n",
    "                              collate_fn=nmt_collate_fn, shuffle=True)\n",
    "nmt_valid_loader = DataLoader(nmt_valid_set, batch_size=batch_size,\n",
    "                              collate_fn=nmt_collate_fn)\n",
    "nmt_test_loader = DataLoader(nmt_test_set, batch_size=batch_size,\n",
    "                             collate_fn=nmt_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NmtModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=512, pad_id=0, hidden_size=512,\n",
    "                 n_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=pad_id)\n",
    "        self.encoder = nn.GRU(embed_size, hidden_size, num_layers=n_layers,\n",
    "                              batch_first=True)\n",
    "        self.decoder = nn.GRU(embed_size, hidden_size, num_layers=n_layers,\n",
    "                              batch_first=True)\n",
    "        self.output = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, pair):\n",
    "        src_embeddings = self.embed(pair.src_token_ids)\n",
    "        tgt_embeddings = self.embed(pair.tgt_token_ids)\n",
    "        src_lengths = pair.src_mask.sum(dim=1)\n",
    "        src_packed = pack_padded_sequence(\n",
    "            src_embeddings, lengths=src_lengths.cpu(),\n",
    "            batch_first=True, enforce_sorted=False)\n",
    "        _, hidden_states = self.encoder(src_packed)\n",
    "        outputs, _ = self.decoder(tgt_embeddings, hidden_states)\n",
    "        return self.output(outputs).permute(0, 2, 1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "vocab_size = nmt_tokenizer.get_vocab_size()\n",
    "nmt_model = NmtModel(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20,                      train loss: 3.4533, train metric: 8.26%, valid metric: 10.13%\n",
      "Epoch 2/20,                      train loss: 2.0985, train metric: 11.29%, valid metric: 10.98%\n",
      "Epoch 3/20,                      train loss: 1.6588, train metric: 12.25%, valid metric: 11.19%\n",
      "Epoch 4/20,                      train loss: 1.4099, train metric: 13.05%, valid metric: 11.24%\n",
      "Epoch 5/20,                      train loss: 1.2552, train metric: 13.34%, valid metric: 11.21%\n",
      "Epoch 6/20,                      train loss: 1.1415, train metric: 13.78%, valid metric: 11.20%\n",
      "Epoch 7/20,                      train loss: 1.0622, train metric: 13.99%, valid metric: 11.14%\n",
      "Epoch 8/20,                      train loss: 0.7658, train metric: 15.24%, valid metric: 11.47%\n",
      "Epoch 9/20,                      train loss: 0.5246, train metric: 16.29%, valid metric: 11.46%\n",
      "Epoch 10/20,                      train loss: 0.4120, train metric: 16.76%, valid metric: 11.43%\n",
      "Epoch 11/20,                      train loss: 0.3468, train metric: 17.03%, valid metric: 11.38%\n",
      "Epoch 12/20,                      train loss: 0.2276, train metric: 17.78%, valid metric: 11.48%\n",
      "Epoch 13/20,                      train loss: 0.1441, train metric: 18.03%, valid metric: 11.47%\n",
      "Epoch 14/20,                      train loss: 0.1159, train metric: 18.28%, valid metric: 11.46%\n",
      "Epoch 15/20,                      train loss: 0.1027, train metric: 18.43%, valid metric: 11.43%\n",
      "Epoch 16/20,                      train loss: 0.0775, train metric: 18.55%, valid metric: 11.46%\n",
      "Epoch 17/20,                      train loss: 0.0647, train metric: 18.31%, valid metric: 11.46%\n",
      "Epoch 18/20,                      train loss: 0.0623, train metric: 18.50%, valid metric: 11.46%\n",
      "Epoch 19/20,                      train loss: 0.0515, train metric: 18.50%, valid metric: 11.48%\n",
      "Epoch 20/20,                      train loss: 0.0478, train metric: 18.35%, valid metric: 11.47%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "xentropy = nn.CrossEntropyLoss(ignore_index=0)  # ignore <pad> tokens\n",
    "optimizer = torch.optim.NAdam(nmt_model.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocab_size)\n",
    "accuracy = accuracy.to(device)\n",
    "\n",
    "history = train(nmt_model, optimizer, xentropy, accuracy,\n",
    "                nmt_train_loader, nmt_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nmt_model.state_dict(), \"my_nmt_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_text, max_length=20, pad_id=0, eos_id=3):\n",
    "    tgt_text = \"\"\n",
    "    token_ids = []\n",
    "    for index in range(max_length):\n",
    "        batch, _ = nmt_collate_fn([{\"sourceString\": src_text,\n",
    "                                    \"targetString\": tgt_text}])\n",
    "        with torch.no_grad():\n",
    "            Y_logits = model(batch.to(device))\n",
    "            Y_token_ids = Y_logits.argmax(dim=1)  # find the best token IDs\n",
    "            next_token_id = Y_token_ids[0, index]  # take the last token ID\n",
    "\n",
    "        next_token = nmt_tokenizer.id_to_token(next_token_id)\n",
    "        tgt_text += \" \" + next_token\n",
    "        if next_token_id == eos_id:\n",
    "            break\n",
    "    return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Me gusta el fútbol . </s>'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_model.eval()\n",
    "translate(nmt_model, \"I like soccer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Me gusta jugar con mis amigos . </s>'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longer_text = \"I like to play soccer with my friends.\"\n",
    "translate(nmt_model, longer_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very basic implementation of beam search. I tried to make it readable and understandable, but it's definitely not optimized for speed! The function first uses the model to find the top _k_ words to start the translations (where _k_ is the beam width). For each of the top _k_ translations, it evaluates the conditional probabilities of all possible words it could add to that translation. These extended translations and their probabilities are added to the list of candidates. Once we've gone through all top _k_ translations and all words that could complete them, we keep only the top _k_ candidates with the highest probability, and we iterate over and over until they all finish with an EOS token. The top translation is then returned (after removing its EOS token).\n",
    "\n",
    "* Note: If p(S) is the probability of sentence S, and p(W|S) is the conditional probability of the word W given that the translation starts with S, then the probability of the sentence S' = concat(S, W) is p(S') = p(S) * p(W|S). As we add more words, the probability gets smaller and smaller. To avoid the risk of it getting too small, which could cause floating point precision errors, the function keeps track of log probabilities instead of probabilities: recall that log(a\\*b) = log(a) + log(b), therefore log(p(S')) = log(p(S)) + log(p(W|S))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, src_text, beam_width=3, max_length=20,\n",
    "                verbose=False, length_penalty=0.6):\n",
    "    top_translations = [(torch.tensor(0.), \"\")]\n",
    "    for index in range(max_length):\n",
    "        if verbose:\n",
    "            print(f\"Top {beam_width} translations so far:\")\n",
    "            for log_proba, tgt_text in top_translations:\n",
    "                print(f\"    {log_proba.item():.3f} – {tgt_text}\")\n",
    "\n",
    "        candidates = []\n",
    "        for log_proba, tgt_text in top_translations:\n",
    "            if tgt_text.endswith(\" </s>\"):\n",
    "                candidates.append((log_proba, tgt_text))\n",
    "                continue  # don't add tokens after EOS token\n",
    "            batch, _ = nmt_collate_fn([{\"sourceString\": src_text,\n",
    "                                        \"targetString\": tgt_text}])\n",
    "            with torch.no_grad():\n",
    "                Y_logits = model(batch.to(device))\n",
    "                Y_log_proba = F.log_softmax(Y_logits, dim=1)\n",
    "                Y_top_log_probas = torch.topk(Y_log_proba, k=beam_width, dim=1)\n",
    "\n",
    "            for beam_index in range(beam_width):\n",
    "                next_token_log_proba = Y_top_log_probas.values[0, beam_index, index]\n",
    "                next_token_id = Y_top_log_probas.indices[0, beam_index, index]\n",
    "                next_token = nmt_tokenizer.id_to_token(next_token_id)\n",
    "                next_tgt_text = tgt_text + \" \" + next_token\n",
    "                candidates.append((log_proba + next_token_log_proba, next_tgt_text))\n",
    "\n",
    "        def length_penalized_score(candidate, alpha=length_penalty):\n",
    "            log_proba, text = candidate\n",
    "            length = len(text.split())\n",
    "            penalty = ((5 + length) ** alpha) / (6 ** alpha)\n",
    "            return log_proba / penalty\n",
    "\n",
    "        top_translations = sorted(candidates,\n",
    "                                  key=length_penalized_score,\n",
    "                                  reverse=True)[:beam_width]\n",
    "\n",
    "    return top_translations[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Me gusta jugar al fútbol con mis amigos . </s>'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_search(nmt_model, longer_text, beam_width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Me gusta jugar con jugar con los jug adores de la playa . </s>'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_text = \"I like to play soccer with my friends at the beach.\"\n",
    "beam_search(nmt_model, longest_text, beam_width=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement Luong attention (a.k.a. dot-product attention):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value):\n",
    "    scores = query @ key.transpose(1, 2)  # [B, Lq, D] @ [B, D, Lk]= [B, Lq, Lk]\n",
    "    weights = torch.softmax(scores, dim=-1)  # [B, Lq, Lk]\n",
    "    return weights @ value  # [B, Lq, Lk] @ [B, Lk, Dv] = [B, Lq, Dv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `B` = batch size\n",
    "* `Lq` = max query length in this batch\n",
    "* `Lk` = max key/value length in this batch (keys and values have the same lengths)\n",
    "* `D` = query embedding size = key embedding size\n",
    "* `Dv` = value embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NmtModelWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=512, pad_id=0, hidden_size=512,\n",
    "                 n_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=pad_id)\n",
    "        self.encoder = nn.GRU(\n",
    "            embed_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.decoder = nn.GRU(\n",
    "            embed_size, hidden_size, num_layers=n_layers, batch_first=True)\n",
    "        self.output = nn.Linear(2 * hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, pair):\n",
    "        src_embeddings = self.embed(pair.src_token_ids)  # same as earlier\n",
    "        tgt_embeddings = self.embed(pair.tgt_token_ids)  # same\n",
    "        src_lengths = pair.src_mask.sum(dim=1)  # same\n",
    "        src_packed = pack_padded_sequence(\n",
    "            src_embeddings, lengths=src_lengths.cpu(),\n",
    "            batch_first=True, enforce_sorted=False)  # same\n",
    "        encoder_outputs_packed, hidden_states = self.encoder(src_packed)\n",
    "        decoder_outputs, _ = self.decoder(tgt_embeddings, hidden_states)  # same\n",
    "        encoder_outputs, _ = pad_packed_sequence(encoder_outputs_packed,\n",
    "                                                 batch_first=True)\n",
    "        attn_output = attention(query=decoder_outputs,\n",
    "                                key=encoder_outputs, value=encoder_outputs)\n",
    "        combined_output = torch.cat((attn_output, decoder_outputs), dim=-1)\n",
    "        return self.output(combined_output).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20,                      train loss: 3.1834, train metric: 8.93%, valid metric: 10.45%\n",
      "Epoch 2/20,                      train loss: 2.0575, train metric: 11.37%, valid metric: 11.12%\n",
      "Epoch 3/20,                      train loss: 1.7671, train metric: 12.01%, valid metric: 11.26%\n",
      "Epoch 4/20,                      train loss: 1.6236, train metric: 12.41%, valid metric: 11.30%\n",
      "Epoch 5/20,                      train loss: 1.5353, train metric: 12.56%, valid metric: 11.35%\n",
      "Epoch 6/20,                      train loss: 1.4763, train metric: 12.83%, valid metric: 11.32%\n",
      "Epoch 7/20,                      train loss: 1.4365, train metric: 12.87%, valid metric: 11.31%\n",
      "Epoch 8/20,                      train loss: 1.4003, train metric: 13.10%, valid metric: 11.32%\n",
      "Epoch 9/20,                      train loss: 1.1272, train metric: 13.76%, valid metric: 11.75%\n",
      "Epoch 10/20,                      train loss: 0.9114, train metric: 14.68%, valid metric: 11.84%\n",
      "Epoch 11/20,                      train loss: 0.8059, train metric: 14.90%, valid metric: 11.86%\n",
      "Epoch 12/20,                      train loss: 0.7432, train metric: 15.27%, valid metric: 11.86%\n",
      "Epoch 13/20,                      train loss: 0.7000, train metric: 15.44%, valid metric: 11.85%\n",
      "Epoch 14/20,                      train loss: 0.6666, train metric: 15.53%, valid metric: 11.86%\n",
      "Epoch 15/20,                      train loss: 0.6363, train metric: 15.74%, valid metric: 11.84%\n",
      "Epoch 16/20,                      train loss: 0.6177, train metric: 15.59%, valid metric: 11.83%\n",
      "Epoch 17/20,                      train loss: 0.5926, train metric: 15.94%, valid metric: 11.82%\n",
      "Epoch 18/20,                      train loss: 0.4580, train metric: 16.58%, valid metric: 11.98%\n",
      "Epoch 19/20,                      train loss: 0.3493, train metric: 17.07%, valid metric: 11.99%\n",
      "Epoch 20/20,                      train loss: 0.2944, train metric: 17.42%, valid metric: 11.99%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "nmt_attn_model = NmtModelWithAttention(vocab_size).to(device)\n",
    "\n",
    "n_epochs = 20\n",
    "xentropy = nn.CrossEntropyLoss(ignore_index=0)  # ignore <pad> tokens\n",
    "optimizer = torch.optim.NAdam(nmt_attn_model.parameters())\n",
    "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=vocab_size)\n",
    "accuracy = accuracy.to(device)\n",
    "\n",
    "history = train(nmt_attn_model, optimizer, xentropy, accuracy,\n",
    "                nmt_train_loader, nmt_valid_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nmt_attn_model.state_dict(), \"my_nmt_attn_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Me gusta jugar fu tbol con mis amigos en la playa . </s>'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(nmt_attn_model, longest_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Material – Exploring Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load BERT's pretrained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "embedding_matrix = model.get_input_embeddings().weight.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a little helper function to get a given token's embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding(token):\n",
    "    token_id = tokenizer.vocab[token]\n",
    "    return embedding_matrix[token_id]\n",
    "\n",
    "get_embedding(\"hello\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes three tokens, computes `E(token2) - E(token1) + E(token3)` (where E(token) is the token's embedding) and finds the most similar token embeddings, using the _cosine similarity_. The cosine similarity between two vectors is the cosine of the angle between the vectors, so its value ranges from –1 (completely opposite) to +1 (perfectly aligned). It returns a list of (similarity, token) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def find_closest_tokens(token1, token2, token3, top_n=5):\n",
    "    E = get_embedding\n",
    "    result = E(token2) - E(token1) + E(token3)\n",
    "    similarities = F.cosine_similarity(result, embedding_matrix)\n",
    "    top_k = torch.topk(similarities, k=top_n)\n",
    "    return [(sim.item(), tokenizer.decode(idx.item()))\n",
    "            for sim, idx in zip(top_k.values, top_k.indices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king is to queen as man is to: man (0.7) woman (0.6) queen (0.5) girl (0.5) lady (0.5) \n",
      "man is to woman as nephew is to: nephew (0.8) niece (0.8) granddaughter (0.7) grandson (0.7) daughters (0.6) \n",
      "father is to mother as son is to: son (0.8) daughter (0.7) mother (0.6) sons (0.5) daughters (0.5) \n",
      "man is to woman as doctor is to: doctor (0.8) doctors (0.6) physician (0.5) woman (0.5) physicians (0.5) \n",
      "germany is to hitler as italy is to: hitler (0.8) mussolini (0.6) italy (0.6) fascism (0.6) italians (0.6) \n",
      "england is to london as germany is to: germany (0.7) london (0.7) berlin (0.6) german (0.5) munich (0.5) \n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    (\"king\", \"queen\", \"man\"),\n",
    "    (\"man\", \"woman\", \"nephew\"),\n",
    "    (\"father\", \"mother\", \"son\"),\n",
    "    (\"man\", \"woman\", \"doctor\"),\n",
    "    (\"germany\", \"hitler\", \"italy\"),\n",
    "    (\"england\", \"london\", \"germany\"),\n",
    "]\n",
    "for (token1, token2, token3) in examples:\n",
    "    print(f\"{token1} is to {token2} as {token3} is to: \", end=\"\")\n",
    "    for similarity, token in find_closest_tokens(token1, token2, token3):\n",
    "        print(f\"{token} ({similarity:.1f})\", end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the correct answer is generally among the closest. That said, if you play around with other examples, you will find that it only works for fairly simple examples: the embeddings aren't always that simple to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Stateless RNNs can only capture patterns whose length is less than, or equal to, the size of the windows the RNN is trained on. Conversely, stateful RNNs can capture longer-term patterns. However, implementing a stateful RNN is much harder⁠—especially preparing the dataset properly. Moreover, stateful RNNs do not always work better, in part because consecutive batches are not independent and identically distributed (IID). Gradient Descent is not fond of non-IID datasets.\n",
    "2. In general, if you translate a sentence one word at a time, the result will be terrible. For example, the French sentence \"Je vous en prie\" means \"You are welcome,\" but if you translate it one word at a time, you get \"I you in pray.\" Huh? It is much better to read the whole sentence first and then translate it. A plain sequence-to-sequence RNN would start translating a sentence immediately after reading the first word, while an Encoder–Decoder RNN will first read the whole sentence and then translate it. That said, one could imagine a plain sequence-to-sequence RNN that would output silence whenever it is unsure about what to say next (just like human translators do when they must translate a live broadcast).\n",
    "3. Variable-length input sequences can be handled by padding the shorter sequences so that all sequences in a batch have the same length, and using masking to ensure the RNN ignores the padding token. For better performance, you may also want to create batches containing sequences of similar sizes. Ragged tensors can hold sequences of variable lengths, and Keras now supports them, which simplifies handling variable-length input sequences (at the time of this writing, it still does not handle ragged tensors as targets on the GPU, though). Regarding variable-length output sequences, if the length of the output sequence is known in advance (e.g., if you know that it is the same as the input sequence), then you just need to configure the loss function so that it ignores tokens that come after the end of the sequence. Similarly, the code that will use the model should ignore tokens beyond the end of the sequence. But generally the length of the output sequence is not known ahead of time, so the solution is to train the model so that it outputs an end-of-sequence token at the end of each sequence.\n",
    "4. Beam search is a technique used to improve the performance of a trained Encoder–Decoder model, for example in a neural machine translation system. The algorithm keeps track of a short list of the _k_ most promising output sentences (say, the top three), and at each decoder step it tries to extend them by one word; then it keeps only the _k_ most likely sentences. The parameter _k_ is called the _beam width_: the larger it is, the more CPU and RAM will be used, but also the more accurate the system will be. Instead of greedily choosing the most likely next word at each step to extend a single sentence, this technique allows the system to explore several promising sentences simultaneously. Moreover, this technique lends itself well to parallelization. You can implement beam search by writing a custom memory cell. Alternatively, TensorFlow Addons's seq2seq API provides an implementation.\n",
    "5. An attention mechanism is a technique initially used in Encoder–Decoder models to give the decoder more direct access to the input sequence, allowing it to deal with longer input sequences. At each decoder time step, the current decoder's state and the full output of the encoder are processed by an alignment model that outputs an alignment score for each input time step. This score indicates which part of the input is most relevant to the current decoder time step. The weighted sum of the encoder output (weighted by their alignment score) is then fed to the decoder, which produces the next decoder state and the output for this time step. The main benefit of using an attention mechanism is the fact that the Encoder–Decoder model can successfully process longer input sequences. Another benefit is that the alignment scores make the model easier to debug and interpret: for example, if the model makes a mistake, you can look at which part of the input it was paying attention to, and this can help diagnose the issue. An attention mechanism is also at the core of the Transformer architecture, in the Multi-Head Attention layers. See the next answer.\n",
    "6. Sampled softmax is used when training a classification model when there are many classes (e.g., thousands). It computes an approximation of the cross-entropy loss based on the logit predicted by the model for the correct class, and the predicted logits for a sample of incorrect words. This speeds up training considerably compared to computing the softmax over all logits and then estimating the cross-entropy loss. After training, the model can be used normally, using the regular softmax function to compute all the class probabilities based on all the logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to build a function that generates strings based on a grammar. The grammar will be represented as a list of possible transitions for each state. A transition specifies the string to output (or a grammar to generate it) and the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_reber_grammar = [\n",
    "    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n",
    "    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
    "    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n",
    "    [(\"T\", 3), (\"V\", 5)], # and so on...\n",
    "    [(\"X\", 3), (\"S\", 6)],\n",
    "    [(\"P\", 4), (\"V\", 6)],\n",
    "    [(\"E\", None)]]        # (state 6) =E=>(terminal state)\n",
    "\n",
    "embedded_reber_grammar = [\n",
    "    [(\"B\", 1)],\n",
    "    [(\"T\", 2), (\"P\", 3)],\n",
    "    [(default_reber_grammar, 4)],\n",
    "    [(default_reber_grammar, 5)],\n",
    "    [(\"T\", 6)],\n",
    "    [(\"P\", 6)],\n",
    "    [(\"E\", None)]]\n",
    "\n",
    "def generate_string(grammar):\n",
    "    state = 0\n",
    "    output = []\n",
    "    while state is not None:\n",
    "        index = np.random.randint(len(grammar[state]))\n",
    "        production, state = grammar[state][index]\n",
    "        if isinstance(production, list):\n",
    "            production = generate_string(grammar=production)\n",
    "        output.append(production)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a few strings based on the default Reber grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_string(default_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now let's generate a few strings based on the embedded Reber grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_string(embedded_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we need a function to generate strings that do not respect the grammar. We could generate a random string, but the task would be a bit too easy, so instead we will generate a string that respects the grammar, and we will corrupt it by changing just one character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSSIBLE_CHARS = \"BEPSTVX\"\n",
    "\n",
    "def generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n",
    "    good_string = generate_string(grammar)\n",
    "    index = np.random.randint(len(good_string))\n",
    "    good_char = good_string[index]\n",
    "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
    "    return good_string[:index] + bad_char + good_string[index + 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few corrupted strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot feed strings directly to an RNN, so we need to encode them somehow. One option would be to one-hot encode each character. Another option is to use embeddings. Let's go for the second option (but since there are just a handful of characters, one-hot encoding would probably be a good option as well). For embeddings to work, we need to convert each string into a sequence of character IDs. Let's write a function for that, using each character's index in the string of possible characters \"BEPSTVX\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_ids(s, chars=POSSIBLE_CHARS):\n",
    "    return [chars.index(c) for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_ids(\"BTTTXXVVETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate the dataset, with 50% good strings, and 50% bad strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(size):\n",
    "    good_strings = [\n",
    "        string_to_ids(generate_string(embedded_reber_grammar))\n",
    "        for _ in range(size // 2)\n",
    "    ]\n",
    "    bad_strings = [\n",
    "        string_to_ids(generate_corrupted_string(embedded_reber_grammar))\n",
    "        for _ in range(size - size // 2)\n",
    "    ]\n",
    "    all_strings = good_strings + bad_strings\n",
    "    X = tf.ragged.constant(all_strings, ragged_rank=1)\n",
    "    y = np.array([[1.] for _ in range(len(good_strings))] +\n",
    "                 [[0.] for _ in range(len(bad_strings))])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, y_train = generate_dataset(10000)\n",
    "X_valid, y_valid = generate_dataset(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first training sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What class does it belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! We are ready to create the RNN to identify good strings. We build a simple sequence binary classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "embedding_size = 5\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n",
    "    tf.keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS),\n",
    "                              output_dim=embedding_size),\n",
    "    tf.keras.layers.GRU(30),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95,\n",
    "                                    nesterov=True)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test our RNN on two tricky strings: the first one is bad while the second one is good. They only differ by the second to last character. If the RNN gets this right, it shows that it managed to notice the pattern that the second letter should always be equal to the second to last letter. That requires a fairly long short-term memory (which is the reason why we used a GRU cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n",
    "                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]\n",
    "X_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)\n",
    "\n",
    "y_proba = model.predict(X_test)\n",
    "print()\n",
    "print(\"Estimated probability that these are Reber strings:\")\n",
    "for index, string in enumerate(test_strings):\n",
    "    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta-da! It worked fine. The RNN found the correct answers with very high confidence. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Work in progress**\n",
    "\n",
    "If you would like to contribute a solution for an exercise, please submit a Pull Request, that would be greatly appreciated. Please aim for simple & flat code with as little boilerplate as possible: optimize for readability rather than efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
